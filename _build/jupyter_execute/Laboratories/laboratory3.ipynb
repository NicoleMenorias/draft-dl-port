{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "24baa06a",
   "metadata": {},
   "source": [
    "# **3 Forward and Backward Propagation (ReLU)**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c7fae12",
   "metadata": {},
   "source": [
    "Instruction: Perform a forward and backward propagation in python using the inputs from Laboratory Task 2\n",
    "\n",
    "`x = np.array([1, 0, 1])\n",
    "y = np.array([1])`\n",
    "\n",
    "use relu as the activation function.\n",
    "\n",
    "`# learning rate\n",
    "lr = 0.001`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "494062da",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Inputs and expected output\n",
    "x = np.array([1, 0, 1])\n",
    "y = np.array([1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "989d4d8a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Learning rate\n",
    "lr = 0.001"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "045dd89b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Initialize weights and bias\n",
    "np.random.seed(42)\n",
    "W = np.random.randn(3, 1)   # 3 inputs â†’ 1 output\n",
    "b = np.random.randn(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d258b878",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Forward Propagation\n",
      "Input: [1 0 1]\n",
      "Weights: [ 0.49671415 -0.1382643   0.64768854]\n",
      "Bias: [1.52302986]\n",
      "Z: [2.66743255]\n",
      "Activated Output (a): [2.66743255]\n",
      "Loss: 2.780331300528872\n"
     ]
    }
   ],
   "source": [
    "# ReLU activation\n",
    "def relu(z):\n",
    "    return np.maximum(0, z)\n",
    "\n",
    "def relu_derivative(z):\n",
    "    return (z > 0).astype(float)\n",
    "\n",
    "# Forward propagation\n",
    "z = np.dot(x, W) + b\n",
    "a = relu(z)\n",
    "\n",
    "# Loss (Mean Squared Error)\n",
    "loss = np.square(y - a).mean()\n",
    "\n",
    "print(\"Forward Propagation\")\n",
    "print(\"Input:\", x)\n",
    "print(\"Weights:\", W.flatten())\n",
    "print(\"Bias:\", b)\n",
    "print(\"Z:\", z)\n",
    "print(\"Activated Output (a):\", a)\n",
    "print(\"Loss:\", loss)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "03199e3d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Backward Propagation\n",
      "dW: [1.66743255 0.         1.66743255]\n",
      "db: [1.66743255]\n",
      "Updated Weights: [ 0.49504672 -0.1382643   0.64602111]\n",
      "Updated Bias: [1.52136242]\n"
     ]
    }
   ],
   "source": [
    "# Backward propagation\n",
    "dz = (a - y) * relu_derivative(z)  # derivative of loss w.r.t z\n",
    "dW = x.reshape(-1, 1) * dz\n",
    "db = dz\n",
    "\n",
    "# Update parameters\n",
    "W -= lr * dW\n",
    "b -= lr * db\n",
    "\n",
    "print(\"\\nBackward Propagation\")\n",
    "print(\"dW:\", dW.flatten())\n",
    "print(\"db:\", db)\n",
    "print(\"Updated Weights:\", W.flatten())\n",
    "print(\"Updated Bias:\", b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b47e9ed",
   "metadata": {},
   "source": [
    "**Observations & Insights**\n",
    "\n",
    "- Training loss shows **large fluctuations** (e.g., ~18 at epoch 300 but >6000 at epoch 500), indicating instability with SGD on small batches.  \n",
    "- Despite fluctuations, the model can occasionally reach **very low training losses (~15)**, meaning it has the capacity to fit the dataset.  \n",
    "- The **final training loss (~14.9)** is much lower than the early epochs, showing the model does learn patterns over time.  \n",
    "- The **final test loss (~2920.8)** is high compared to training loss, suggesting **overfitting** and poor generalization.  \n",
    "- Using the Diabetes dataset (small sample size, noisy target) contributes to unstable convergence.  \n",
    "- Two fully connected layers give the model more flexibility, but with **SGD (no momentum)** and **batch size of 8**, the optimization remains noisy.  \n",
    "- Overall: the model fits training data but struggles to generalize, highlighting dataset limitations and optimizer instability."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "cardio",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.23"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}